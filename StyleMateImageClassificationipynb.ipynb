{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "1B6RT4RY16iP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the datasets from gdrive into /content/\n",
        "!unzip /content/drive/MyDrive/style-mate/datasets/clean_3class_320px_new.zip -d /content/"
      ],
      "metadata": {
        "id": "Cp5XpQq12odZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !file /content/augmented/test/Heart/Heart__0_100.jpg"
      ],
      "metadata": {
        "id": "Fq2iPtkv4OAi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "batch_size = 32\n",
        "img_height = 320\n",
        "img_width = 320\n",
        "num_classes = 3\n",
        "epochs = 20\n",
        "\n",
        "datasets_dir = \"/content/result\"\n",
        "\n",
        "# Create an ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.15,\n",
        "    rotation_range=4,\n",
        "    width_shift_range=0.04,\n",
        "    height_shift_range=0.04\n",
        "    )\n",
        "\n",
        "## Load and augment training data\n",
        "#train_data_gen = datagen.flow_from_directory(datasets_dir + 'train',\n",
        "#                                             target_size=(img_height, img_width),\n",
        "#                                             batch_size=batch_size,\n",
        "#                                             class_mode='sparse')\n",
        "#                                             #color_mode='grayscale')\n",
        "#\n",
        "## Load validation data\n",
        "#val_data_gen = datagen.flow_from_directory(datasets_dir + 'test',\n",
        "#                                           target_size=(img_height, img_width),\n",
        "#                                           batch_size=batch_size,\n",
        "#                                           class_mode='sparse')\n",
        "#                                           #color_mode='grayscale')\n",
        "#\n",
        "## Load MobileNetV2 as a pretrained model\n",
        "#base_model = tf.keras.applications.MobileNetV2(input_shape=(img_height, img_width, 3),\n",
        "#                                               include_top=False,\n",
        "#                                               weights='imagenet')\n",
        "#base_model.trainable = False\n",
        "#\n",
        "\n",
        "train_generator = datagen.flow_from_directory(datasets_dir,\n",
        "                                              target_size=(img_height, img_width),\n",
        "                                              batch_size=batch_size,\n",
        "                                              class_mode='sparse',\n",
        "                                              subset='training'\n",
        "                                              )\n",
        "\n",
        "valid_generator = datagen.flow_from_directory(datasets_dir,\n",
        "                                              target_size=(img_height, img_width),\n",
        "                                              batch_size=batch_size,\n",
        "                                              class_mode='sparse',\n",
        "                                              subset='validation'\n",
        "                                              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTvhMCx91_Pf",
        "outputId": "df9169eb-6294-49dd-b583-62cf8287f9f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 431 images belonging to 3 classes.\n",
            "Found 74 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your model on top of MobileNetV2\n",
        "#model = models.Sequential([\n",
        "#    base_model,\n",
        "#    layers.GlobalAveragePooling2D(),\n",
        "#    layers.Dropout(0.25),\n",
        "#    layers.Dense(25, activation='relu'),\n",
        "#    layers.Dense(num_classes, activation='softmax')\n",
        "#])\n",
        "#\n",
        "## Compile the model with sparse categorical entropy loss\n",
        "#model.compile(optimizer='adam',\n",
        "#              loss='sparse_categorical_crossentropy',\n",
        "#              metrics=['accuracy'])\n",
        "#model=tf.keras.models.Sequential([\n",
        "#        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(320, 320, 3)),\n",
        "#        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "#        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "#        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "#        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "#        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "#        tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "#        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "#        tf.keras.layers.Conv2D(512, (3, 3), activation='relu'),\n",
        "#        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "#        tf.keras.layers.Flatten(),\n",
        "#        tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=0.00003)),\n",
        "#        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "#    ])\n",
        "\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(img_height, img_width, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# model.fit(training_generator, epochs=3)\n"
      ],
      "metadata": {
        "id": "9xip587P2CjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a740ed-0c58-4f80-92b3-a5d57fbd73a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('/content/style_mate_image_classification.h5', save_weights_only=True, save_best_only=True)"
      ],
      "metadata": {
        "id": "ziwQQnYHoJVa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuUU5F_i1xk_",
        "outputId": "c9105595-988b-4882-ff7c-a2d39c928a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "14/14 [==============================] - 56s 4s/step - loss: 1.1189 - accuracy: 0.4385 - val_loss: 1.0467 - val_accuracy: 0.5270\n",
            "Epoch 2/25\n",
            "14/14 [==============================] - 52s 4s/step - loss: 0.9610 - accuracy: 0.5545 - val_loss: 1.1303 - val_accuracy: 0.5135\n",
            "Epoch 3/25\n",
            "14/14 [==============================] - 49s 4s/step - loss: 0.8331 - accuracy: 0.5986 - val_loss: 1.2157 - val_accuracy: 0.5541\n",
            "Epoch 4/25\n",
            "14/14 [==============================] - 52s 4s/step - loss: 0.7481 - accuracy: 0.6705 - val_loss: 1.2762 - val_accuracy: 0.5270\n",
            "Epoch 5/25\n",
            "14/14 [==============================] - 49s 4s/step - loss: 0.7012 - accuracy: 0.6937 - val_loss: 1.2535 - val_accuracy: 0.5405\n",
            "Epoch 6/25\n",
            "14/14 [==============================] - 48s 3s/step - loss: 0.6916 - accuracy: 0.6961 - val_loss: 1.2577 - val_accuracy: 0.5405\n",
            "Epoch 7/25\n",
            "14/14 [==============================] - 48s 3s/step - loss: 0.6837 - accuracy: 0.7030 - val_loss: 1.2914 - val_accuracy: 0.5676\n",
            "Epoch 8/25\n",
            "14/14 [==============================] - 48s 3s/step - loss: 0.6342 - accuracy: 0.7401 - val_loss: 1.3389 - val_accuracy: 0.6081\n",
            "Epoch 9/25\n",
            "14/14 [==============================] - 53s 4s/step - loss: 0.5747 - accuracy: 0.7819 - val_loss: 1.2772 - val_accuracy: 0.5270\n",
            "Epoch 10/25\n",
            "14/14 [==============================] - 48s 3s/step - loss: 0.5937 - accuracy: 0.7401 - val_loss: 1.2854 - val_accuracy: 0.5541\n",
            "Epoch 11/25\n",
            "14/14 [==============================] - 48s 3s/step - loss: 0.5574 - accuracy: 0.7425 - val_loss: 1.3829 - val_accuracy: 0.5405\n",
            "Epoch 12/25\n",
            "14/14 [==============================] - 48s 3s/step - loss: 0.5684 - accuracy: 0.7494 - val_loss: 1.2722 - val_accuracy: 0.5676\n",
            "Epoch 13/25\n",
            "14/14 [==============================] - 50s 4s/step - loss: 0.5239 - accuracy: 0.7958 - val_loss: 1.3027 - val_accuracy: 0.5405\n",
            "Epoch 14/25\n",
            "14/14 [==============================] - 53s 4s/step - loss: 0.5335 - accuracy: 0.7842 - val_loss: 1.2850 - val_accuracy: 0.5135\n",
            "Epoch 15/25\n",
            "14/14 [==============================] - 50s 4s/step - loss: 0.4839 - accuracy: 0.7981 - val_loss: 1.3716 - val_accuracy: 0.5270\n",
            "Epoch 16/25\n",
            "14/14 [==============================] - 48s 3s/step - loss: 0.4905 - accuracy: 0.8097 - val_loss: 1.3761 - val_accuracy: 0.5676\n",
            "Epoch 17/25\n",
            "14/14 [==============================] - 53s 4s/step - loss: 0.4738 - accuracy: 0.8028 - val_loss: 1.2932 - val_accuracy: 0.5946\n",
            "Epoch 18/25\n",
            "14/14 [==============================] - 48s 3s/step - loss: 0.4657 - accuracy: 0.8097 - val_loss: 1.3310 - val_accuracy: 0.5405\n",
            "Epoch 19/25\n",
            "14/14 [==============================] - 52s 4s/step - loss: 0.4719 - accuracy: 0.8051 - val_loss: 1.3456 - val_accuracy: 0.5676\n",
            "Epoch 20/25\n",
            "14/14 [==============================] - 50s 4s/step - loss: 0.4474 - accuracy: 0.8260 - val_loss: 1.2603 - val_accuracy: 0.5811\n",
            "Epoch 21/25\n",
            "14/14 [==============================] - 53s 4s/step - loss: 0.4653 - accuracy: 0.8074 - val_loss: 1.3421 - val_accuracy: 0.5811\n",
            "Epoch 22/25\n",
            "14/14 [==============================] - 50s 4s/step - loss: 0.4397 - accuracy: 0.8097 - val_loss: 1.3918 - val_accuracy: 0.5811\n",
            "Epoch 23/25\n",
            "14/14 [==============================] - 50s 4s/step - loss: 0.4313 - accuracy: 0.8237 - val_loss: 1.3694 - val_accuracy: 0.5676\n",
            "Epoch 24/25\n",
            "14/14 [==============================] - 48s 3s/step - loss: 0.4431 - accuracy: 0.8306 - val_loss: 1.4318 - val_accuracy: 0.5676\n",
            "Epoch 25/25\n",
            "14/14 [==============================] - 53s 4s/step - loss: 0.4235 - accuracy: 0.8399 - val_loss: 1.3253 - val_accuracy: 0.6351\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a6e84562ec0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_generator, epochs=25, validation_data=valid_generator) #, callbacks=[checkpoint_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "# Define your label names based on your model's classes\n",
        "class_labels = ['heart', 'oblong', 'square']\n",
        "\n",
        "# Assuming you have your model defined as 'model'\n",
        "img_path = '/content/heart_lmao.jpg'  # Change this to the path of your test image\n",
        "img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = preprocess_input(img_array)\n",
        "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "# Make a prediction\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# Adjust the decoding logic based on the number of classes in your model\n",
        "def decode_face_shape_predictions(predictions, class_labels):\n",
        "    top_prediction_idx = np.argmax(predictions, axis=1)\n",
        "    face_shape_prediction = class_labels[top_prediction_idx[0]]\n",
        "    confidence = predictions[0][top_prediction_idx][0]\n",
        "    return face_shape_prediction, confidence\n",
        "\n",
        "# Decode predictions\n",
        "predicted_face_shape, confidence = decode_face_shape_predictions(predictions, class_labels)\n",
        "\n",
        "# Display the prediction\n",
        "print(f\"Predicted Face Shape: >> {predicted_face_shape} << with Confidence: {confidence:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx-dHSFb4gzP",
        "outputId": "fe90c9fa-a0e2-48dd-cefa-d07b61f6d44b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 76ms/step\n",
            "Predicted Face Shape: >> oblong << with Confidence: 0.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.saving.save_model(model, '/content/drive/MyDrive/style-mate/saved_model')"
      ],
      "metadata": {
        "id": "yaDmZyM9AH6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de093b2-d47d-4637-b186-af9c786d47dd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
            "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
            "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/MyDrive/style-mate/saved_model')"
      ],
      "metadata": {
        "id": "aykaTx6QOQ45"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('/content/drive/MyDrive/style-mate/style_mate_classification.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "EZ3ndezDJWVH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "id": "epxr3sHrJkPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=keras  /content/drive/MyDrive/style-mate/style_mate_image_classification_new.h5 /content/drive/MyDrive/style-mate/tfjs_model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvzmSA_pKD9Y",
        "outputId": "43abe21e-05f2-4675-9a64-0f8dcd00db2a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-06 08:08:06.515195: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-06 08:08:06.515267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-06 08:08:06.517104: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-06 08:08:07.891805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2pNKh3zBKacD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}